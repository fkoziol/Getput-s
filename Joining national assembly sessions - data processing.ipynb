{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter as counter\n",
    "from stop_words import get_stop_words\n",
    "import nltk\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import csv\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from nltk.tokenize import MWETokenizer\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These informations must me changed depending on your file path of course\n",
    "path = r'C:\\Users\\Florent\\Documents\\Code\\Deputes'\n",
    "files = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "files.sort()\n",
    "data_files = files[6:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the most obvious stop words\n",
    "stop_words = get_stop_words('fr')\n",
    "stop_words += ','\n",
    "stop_words += '.'\n",
    "stop_words += ':'\n",
    "stop_words += \"''\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file\n",
      "file\n",
      "file\n",
      "file\n",
      "file\n",
      "file\n",
      "file\n",
      "file\n",
      "file\n",
      "file\n",
      "file\n",
      "file\n",
      "file\n",
      "file\n",
      "file\n",
      "file\n",
      "file\n",
      "file\n",
      "file\n",
      "file\n",
      "file\n",
      "file\n",
      "file\n",
      "file\n",
      "file\n",
      "file\n",
      "file\n",
      "file\n",
      "file\n",
      "file\n",
      "file\n",
      "file\n",
      "file\n",
      "file\n",
      "file\n",
      "file\n",
      "file\n",
      "file\n",
      "file\n",
      "file\n",
      "file\n",
      "file\n",
      "file\n",
      "file\n",
      "file\n",
      "file\n",
      "file\n",
      "file\n",
      "file\n",
      "file\n",
      "file\n",
      "file\n",
      "file\n",
      "file\n",
      "file\n",
      "file\n"
     ]
    }
   ],
   "source": [
    "#For all file, creating a dictionary with each intervention text added to the corresponding deputy\n",
    "with open(data_files[0]) as read:\n",
    "    json_file = json.load(read)\n",
    "\n",
    "depute_texte_dict = {}\n",
    "for file in data_files:\n",
    "    read = open(file)\n",
    "    json_file = json.load(read)\n",
    "    \n",
    "    for key in json_file.keys():\n",
    "        for sub_key in json_file.get(key).keys():\n",
    "            for intervention in json_file.get(key).get(sub_key)['interventions']:\n",
    "                \n",
    "                if intervention['fonction_intervenant'] != 'président':\n",
    "                    tokens = nltk.word_tokenize(intervention['texte'])\n",
    "                    tokens_without_sw = [word.lower() for word in tokens if not word.lower() in stop_words]\n",
    "                    \n",
    "                    for token in tokens_without_sw:\n",
    "                        if intervention['intervenant'] in depute_texte_dict:\n",
    "                            depute_texte_dict[intervention['intervenant']] += ' ' \n",
    "                            depute_texte_dict[intervention['intervenant']] += token\n",
    "                        \n",
    "                        else:\n",
    "                            depute_texte_dict[intervention['intervenant']] = ' ' \n",
    "                            depute_texte_dict[intervention['intervenant']] += token\n",
    "    \n",
    "    print('file')\n",
    "    read.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the file\n",
    "df_clean = pd.read_csv('depute_texte_counter.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizing most usual word combinations that don't add anything to an NLP analysis\n",
    "tokenizer = MWETokenizer([(\"monsieur\",\"ministre\"),\n",
    "                          (\"chers\",\"collègue\"),\n",
    "                          (\"avis\",\"défavorable\"),\n",
    "                          (\"projet\",\"loi\"),\n",
    "                          (\"madame\",\"ministre\"),\n",
    "                          (\"secrétaire\",\"d'état\"),\n",
    "                          (\"proposition\",\"loi\"),\n",
    "                          (\"monsieur\",\"président\"),\n",
    "                         ('monsieur', 'rapporteur'),\n",
    "                         ('madame', 'présidente')], separator=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listing custom stop words found with some data processing aside from this script\n",
    "stop_words = ['monsieur ministre', \n",
    "              'chers collègues',\n",
    "             'avis défavorable',\n",
    "             'projet loi',\n",
    "             'madame ministre',\n",
    "             \"secrétaire d'état\",\n",
    "             'proposition loi',\n",
    "             'monsieur président',\n",
    "             'chers collègues',\n",
    "             'effet',\n",
    "             'monsieur rapporteur',\n",
    "             'madame présidente',\n",
    "             \"c'est\",\n",
    "             '–',\n",
    "             'plus'\n",
    "             'bien',\n",
    "             \"d'un\",\n",
    "             \"d'une\",\n",
    "             \"qu'il\",\n",
    "             \"n'est\",\n",
    "             ';',\n",
    "             'm.',\n",
    "             'loi',\n",
    "             'amendement',\n",
    "             'texte',\n",
    "             'faut',\n",
    "             'entre',\n",
    "             'déjà',\n",
    "             \"l'article\",\n",
    "             \"l'amendement\",\n",
    "             'notamment',\n",
    "             'cas',\n",
    "             'monsieur',\n",
    "             'madame',\n",
    "             'également'\n",
    "             'mme',\n",
    "             'temps',\n",
    "             'amendements'\n",
    "             'afin',\n",
    "             \"s'agit\",\n",
    "             'président',\n",
    "             'no',\n",
    "             'lors',\n",
    "             'rapport',\n",
    "             'plusieurs',\n",
    "             \"d'autres\",\n",
    "             \"qu'ils\",\n",
    "             'or',\n",
    "             'trois',\n",
    "             'celui',\n",
    "             'quelques',\n",
    "             \"prendre\",\n",
    "             \"d'ailleurs\",\n",
    "             'réforme',\n",
    "             'celle',\n",
    "             'chers',\n",
    "             'tel',\n",
    "             'dont',\n",
    "             'plu',\n",
    "             'plus',\n",
    "             'bien',\n",
    "             'ainsi',\n",
    "             'non',\n",
    "             \"aujourd'hui\",\n",
    "             'question',\n",
    "             'france',\n",
    "             'qu',\n",
    "             'si',\n",
    "             'on',\n",
    "             'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Florent\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "C:\\Users\\Florent\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#Deleting the new stop words \n",
    "#Adding a column with a dictionary giving the words from their vocabulary as a key, and the number of times used as value\n",
    "for i in range(810):\n",
    "    long_string = df_clean['texte'][i]\n",
    "    tokens = tokenizer.tokenize(long_string.split())\n",
    "    tokens_without_sw = [word for word in tokens if not word in stop_words]\n",
    "    df_clean['texte'][i] = ' '.join(tokens_without_sw)\n",
    "    df_clean['counter'][i] = dict(sorted(Counter(tokens_without_sw).items(), key=lambda item: item[1], reverse = True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For a reason I don't really get, the column with the names of the deputies is duplicated everytime and given a default name\n",
    "#Change the name of the first column in this script with the name you have on your own dataframe\n",
    "df_clean = df_clean[['Unnamed: 0.1.1.1', 'texte', 'counter']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the same reason, change the name of the selected column with the name of the column containing the deputies names\n",
    "#This remove any special character and replace it with the \"normal\" character (ex: \"ê\" becomes \"e\")\n",
    "for i in range(810):\n",
    "    df_clean['Unnamed: 0.1.1.1'][i] = unidecode.unidecode(df_clean['Unnamed: 0.1.1.1'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.to_csv('depute_texte_counter.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
